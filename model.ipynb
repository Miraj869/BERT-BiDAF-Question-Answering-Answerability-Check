{"cells":[{"cell_type":"markdown","metadata":{"id":"wJzuqd7PYrYY"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55564,"status":"ok","timestamp":1676065175012,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"8qNhPt6hmdB0","outputId":"5d802d95-79f3-40f6-de2c-487c17066761"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-text==2.9.0\n","  Downloading tensorflow_text-2.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text==2.9.0) (0.12.0)\n","Collecting tensorflow<2.10,>=2.9.0\n","  Downloading tensorflow-2.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.14.1)\n","Collecting keras-preprocessing>=1.1.1\n","  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.3.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (23.0)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.0)\n","Collecting keras<2.10.0,>=2.9.0rc0\n","  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.19.6)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (15.0.6.1)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.1.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (57.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.30.0)\n","Collecting flatbuffers<2,>=1.12\n","  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.6.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.15.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.51.1)\n","Collecting tensorboard<2.10,>=2.9\n","  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.21.6)\n","Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n","  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 KB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.38.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.25.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.0.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.16.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.4.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.2.8)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (5.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (6.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2022.12.7)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.12.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text==2.9.0) (3.2.2)\n","Installing collected packages: keras, flatbuffers, tensorflow-estimator, keras-preprocessing, tensorboard, tensorflow, tensorflow-text\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.11.0\n","    Uninstalling keras-2.11.0:\n","      Successfully uninstalled keras-2.11.0\n","  Attempting uninstall: flatbuffers\n","    Found existing installation: flatbuffers 23.1.21\n","    Uninstalling flatbuffers-23.1.21:\n","      Successfully uninstalled flatbuffers-23.1.21\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.11.0\n","    Uninstalling tensorflow-estimator-2.11.0:\n","      Successfully uninstalled tensorflow-estimator-2.11.0\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.11.2\n","    Uninstalling tensorboard-2.11.2:\n","      Successfully uninstalled tensorboard-2.11.2\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.11.0\n","    Uninstalling tensorflow-2.11.0:\n","      Successfully uninstalled tensorflow-2.11.0\n","Successfully installed flatbuffers-1.12 keras-2.9.0 keras-preprocessing-1.1.2 tensorboard-2.9.1 tensorflow-2.9.3 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"]}],"source":["!pip install transformers\n","!pip install tensorflow-text==2.9.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22786,"status":"ok","timestamp":1676065197791,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"TYGS9asZmoiW","outputId":"9e776b5c-e0f3-43da-dc62-3c37029b086f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLFKz22kYrYZ"},"outputs":[],"source":["from tensorflow.keras.layers import Input, GRU, Dense, Embedding, Bidirectional, TimeDistributed   #layers required for network\n","from tensorflow.keras.layers import Layer, Conv1D, Softmax, Concatenate ,Dropout, MaxPool1D        #layers required for network\n","from tensorflow.keras.backend import expand_dims, tile, concatenate, shape, batch_dot, squeeze     #functions required for network\n","import tensorflow.keras.backend as K                                                               #to build metric\n","from tensorflow.keras.models import Model                                                          #to build model\n","from tensorflow.keras.callbacks import TensorBoard                                                 #tensorboard\n","import tensorflow as tf                                                                            #other functions\n","from tqdm import tqdm                                                                              #track progress\n","import numpy as np                                                                                 #for numpy operations\n","import pickle                                                                            #loading tokenizers\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from transformers import BertTokenizer\n","import math\n","from numpy import save, load\n","import tensorflow.nn as nn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":939,"status":"ok","timestamp":1676065201344,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"FRZJIIOAYrYd","outputId":"c1d134db-acea-4907-885d-51e023a73e3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","2.9.3\n"]}],"source":["print(tf.executing_eagerly())\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1676065201346,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"pzMKM8ccjghp","outputId":"991ec6ad-ebb2-41e0-9da0-a12818d5bfea"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(tf.config.list_physical_devices('GPU'))"]},{"cell_type":"markdown","metadata":{"id":"axA7pZwshL0R"},"source":["## Loading all the Required Variables from disk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dU5bvso-ufs3"},"outputs":[],"source":["# y_start_train_a = load(\"/content/y_start_train.npy\")\n","# y_end_train_a = load(\"/content/y_end_train.npy\")\n","\n","# train_input_id_a = load(\"/content/ids_train (1).npy\")\n","# train_input_mask_a = load(\"/content/mask_train (1).npy\")\n","# train_input_type_a = load(\"/content/type_train (1).npy\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o-wRpo91rt1R"},"outputs":[],"source":["# Loading Data\n","y_start_train = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/y_start_train.npy\")\n","y_end_train = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/y_end_train.npy\")\n","y_end_test = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/y_end_test.npy\")\n","y_start_test = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/y_start_test.npy\")\n","\n","train_input_id = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/ids_train.npy\")\n","train_input_mask = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/mask_train.npy\")\n","train_input_type = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/type_train.npy\")\n","\n","test_input_id = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/ids_test.npy\")\n","test_input_mask = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/mask_test.npy\")\n","test_input_type = load(\"/content/gdrive/MyDrive/Colab Notebooks/InterIIT Dataset/type_test.npy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1676065218415,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"dNs2ZE_pLNHF","outputId":"6317ac79-ec8a-4a33-e8da-68d1c32c7306"},"outputs":[{"data":{"text/plain":["(60016, 461)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["y_start_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKIIgKfXwAQj"},"outputs":[],"source":["train_input_id = tf.convert_to_tensor(train_input_id,dtype=tf.int32)\n","train_input_mask = tf.convert_to_tensor(train_input_mask,dtype=tf.int32)\n","train_input_type = tf.convert_to_tensor(train_input_type,dtype=tf.int32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676065219476,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"C87SqcNq88V4","outputId":"0b4d9cd9-70c1-4ee7-e427-4ed2336c00c4"},"outputs":[{"data":{"text/plain":["TensorShape([60016, 512])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_input_type.shape"]},{"cell_type":"markdown","metadata":{"id":"1yCTcsN2DUlt"},"source":["# Making Layers"]},{"cell_type":"markdown","metadata":{"id":"ykCh_RWxEGiB"},"source":["## Bert Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gATUv72FEMQ4"},"outputs":[],"source":["class Bert(Layer):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        input_id,input_mask,input_type = inputs\n","        outputs = self.bert_encoder({'input_mask': input_mask,'input_type_ids': input_type,'input_word_ids': input_id},False)\n","        context_features = outputs['sequence_output'][:, 0:461,:]\n","        question_features = tf.concat((tf.expand_dims(outputs['sequence_output'][:, 0, :],axis=1), outputs['sequence_output'][:, 461:, :]), axis=1)\n","        return question_features,context_features"]},{"cell_type":"markdown","metadata":{"id":"8x4XG5gvEEr6"},"source":["## Attention Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvOhH5B0EMQ5"},"outputs":[],"source":["class AttFlow(Layer):\n","\n","    def __init__(self, feature_dimension):\n","        super().__init__()\n","        \n","        #output shape: 1, input shape: 3*Dim\n","        self.weight = Dense(1,input_shape=(3*feature_dimension,), activation=None, use_bias=False)\n","    \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        context_features, question_features = inputs\n","        # Construct a similarity matrix\n","        batch_size = 16          # N\n","        feature_dimension = 768   # d\n","        length_context = 461     # T\n","        length_quesiton = 52    # J\n","\n","        # if batch_size == None:\n","        #     batch_size = 0\n","\n","        shape = (batch_size, length_context, length_quesiton,feature_dimension)    # (N,T,J,d)\n","        \n","        #INITIALLY context_features: (N,T,d), question features: (N,J,d)\n","\n","        context_features_expanded = tf.expand_dims(context_features,2)                  # (N,T,1,d)\n","        # print(context_features_expanded.shape)\n","        broadcast_shape = tf.where([True, False, False, False],tf.shape(context_features_expanded), [batch_size, length_context, length_quesiton,feature_dimension])\n","        context_features_expanded = tf.broadcast_to(context_features_expanded,broadcast_shape)    # (N,T,J,d)\n","        \n","        question_features_expanded = tf.expand_dims(question_features,1)                # (N,1,J,d)\n","        broadcast_shape = tf.where([True, False, False, False],tf.shape(question_features_expanded), [batch_size, length_context, length_quesiton,feature_dimension])\n","        question_features_expanded = tf.broadcast_to(question_features_expanded,broadcast_shape)  # (N,T,J,d)\n","\n","        entrywise_prod = tf.math.multiply(context_features_expanded, question_features_expanded)   # (N,T,J,d)\n","        \n","        concat_feature = tf.concat( [context_features_expanded, question_features_expanded, entrywise_prod], axis=-1)  # (N,T,J,3d)\n","        \n","        broadcast_shape = tf.where([True, False, False],[16, length_context, length_quesiton], [batch_size, length_context, length_quesiton])\n","        similarity = tf.reshape(self.weight(concat_feature) ,broadcast_shape)  # (N,T,J)\n","\n","        # Context2Question attention\n","        weight_c2q = nn.softmax(similarity, axis=-1) # (N,T,J)\n","\n","        # (N,T,J) * (N,J,d) -> (N,T,d)\n","        c2q = tf.matmul(weight_c2q, question_features)\n","\n","        # Question2Context attention\n","        weight_q2c = nn.softmax( tf.reduce_max(similarity, axis=[2]), axis=-1)  # (N,T)\n","\n","        # (N,1,T) * (N,T,d) -> (N,1,d)\n","        q2c = tf.matmul(tf.expand_dims(weight_q2c, 1), context_features)\n","\n","        q2c = tf.tile(q2c, [1, length_context,1])   # (N,T,d)\n","\n","        return c2q, q2c"]},{"cell_type":"markdown","metadata":{"id":"jkcqIOqxYrZU"},"source":["### Modelling Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYglE4kmYrZV"},"outputs":[],"source":["class modelling_input_layer(Layer):\n","    \n","    def __init__(self):\n","        \n","        super(modelling_input_layer, self).__init__()\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        \n","        H, c2q, q2c = inputs\n","        G = concatenate([H, c2q, (H*c2q), (H*q2c)], axis=2)\n","        \n","        return G"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrBv61sjYrZW"},"outputs":[],"source":["class modelling_layer(Layer):\n","    \n","    def __init__(self, output_dim):\n","        \n","        super(modelling_layer, self).__init__()\n","        self.output_dim = output_dim\n","        self.modelling1 = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2))\n","        self.modelling2 = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2))\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        return self.modelling2(self.modelling1(inputs))\n","    \n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'output_dim': self.output_dim,\n","        })\n","        \n","        return config"]},{"cell_type":"markdown","metadata":{"id":"EuTXgszVYrZY"},"source":["### Output Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aO_mKCeYrZY"},"outputs":[],"source":["class input_to_start(Layer):\n","    \n","    def __init__(self):\n","        \n","        super(input_to_start, self).__init__()\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        \n","        G, M = inputs\n","        GM = concatenate([G, M], axis=2)\n","        \n","        return GM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJfh8XQsYrZa"},"outputs":[],"source":["class output_start(Layer):\n","    \n","    def __init__(self):\n","        \n","        super(output_start, self).__init__()\n","        self.dense = Dense(1, activation = \"linear\", kernel_initializer=tf.keras.initializers.glorot_uniform(seed=35))\n","        self.dropout = Dropout(0.2)\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        \n","        GM = inputs\n","        start = self.dense(GM)\n","        start = self.dropout(start)\n","        p1 = tf.nn.softmax(squeeze(start, axis=2))\n","        \n","        return p1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DiNgCf2YrZb"},"outputs":[],"source":["class input_to_end(Layer):\n","    \n","    def __init__(self, output_dim):\n","        \n","        super(input_to_end, self).__init__()\n","        self.output_dim = output_dim\n","        self.end = Bidirectional(GRU(self.output_dim, return_sequences=True, dropout=0.2, kernel_initializer=tf.keras.initializers.glorot_uniform(seed=5)))\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        \n","        G, M = inputs\n","        M2 = self.end(M)\n","        GM2 = concatenate([G, M2], axis=2)\n","        return GM2\n","    \n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'output_dim': self.output_dim,\n","        })\n","        \n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xY4PmUS3YrZd"},"outputs":[],"source":["class output_end(Layer):\n","    \n","    def __init__(self):\n","        \n","        super(output_end, self).__init__()\n","        self.dense = Dense(1, activation = \"linear\", kernel_initializer=tf.keras.initializers.glorot_uniform(seed=85))\n","        self.dropout = Dropout(0.2)\n","        \n","    def build(self, input_shape):\n","        self.built = True\n","\n","    def call(self, inputs):\n","        \n","        GM2 = inputs\n","        end = self.dense(GM2)\n","        end = self.dropout(end)\n","        p2 = tf.nn.softmax(squeeze(end, axis=2))\n","        \n","        return p2"]},{"cell_type":"markdown","metadata":{"id":"U4N1JgAKYrZe"},"source":["## Define Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X3peeA-8YrZg"},"outputs":[],"source":["def bidaf_model(hidden_size,  n_filters, filter_width):\n","    \"\"\"Function that build the BiDAF model using the custom class layers\"\"\"\n","    \n","    #inputs\n","    input_id = Input(shape=(512),name = 'Ids',dtype=tf.int32)\n","    input_mask = Input(shape=(512),name = 'mask',dtype=tf.int32)\n","    input_type = Input(shape=(512),name = 'Type',dtype=tf.int32)\n","    # question_words = Input(shape=(question_timesteps,), name = 'question_word_tokens')\n","    # context_words = Input(shape=(context_timesteps,), name = 'context_word_tokens')    \n","    # question_chars = Input(shape=(question_timesteps,char_max,), name = 'question_char_tokens')\n","    # context_chars = Input(shape=(context_timesteps,char_max,), name = 'context_char_tokens')\n","\n","    # #word embedding layer\n","    # question_word_embedded, context_word_embedded = word_embedding_layer(len(word_tokenizer)+1, hidden_size, question_words.shape[-1])([question_words, context_words])\n","            \n","    # #character embedding layer\n","    # question_char_embedded, context_char_embedded = char_embedding_layer(len(char_tokenizer.word_index)+1, char_vocab, question_chars.shape[-1])([question_chars, context_chars])\n","    \n","    # #character CNN\n","    # question_char_embedded, context_char_embedded  = char_cnn_layer(n_filters, filter_width)([question_char_embedded, context_char_embedded])\n","    \n","    # #highway layer\n","    # context, question = highway_input_layer()([question_word_embedded, context_word_embedded, question_char_embedded, context_char_embedded])\n","    # question_blendrep = highway_layer(\"question_highway\")(question)\n","    # context_blendrep = highway_layer(\"context_highway\")(context)\n","\n","    # #contextual layer\n","    # question_contextual = contextual_layer(hidden_size, \"question_contextual\")(question_blendrep)\n","    # context_contextual = contextual_layer(hidden_size, \"context_contextual\")(context_blendrep)\n","    \n","    \n","    Question_features,Context_features = Bert() ([input_id,input_mask,input_type])\n","\n","    #attention layer\n","    c2q,q2c = AttFlow(768)([Context_features, Question_features])\n","    # attention_in = attention_input_layer()([context_contextual, question_contextual])\n","    # attention_out = attention_layer()(attention_in)\n","    # c2q, q2c = c2q_q2c_layer()([attention_out, context_contextual, question_contextual])\n","    \n","    #modelling layer\n","    G = modelling_input_layer()([Context_features, c2q, q2c])\n","    M = modelling_layer(hidden_size)(G)\n","    \n","    #output layers\n","    GM = input_to_start()([G,M])\n","    start = output_start()(GM)\n","    GM2 = input_to_end(hidden_size)([G,M])\n","    end = output_end()(GM2)\n","\n","    model = Model(inputs=[input_id,input_mask,input_type], outputs=[start, end], name=\"bidaf\")\n","        \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"PBknca-MYrZk"},"source":["### Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12625,"status":"ok","timestamp":1675611801866,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"YjnZWESDYrZw","outputId":"742fe0cb-c422-466f-c288-ce1dc4a5324e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"bidaf\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Ids (InputLayer)               [(None, 512)]        0           []                               \n","                                                                                                  \n"," mask (InputLayer)              [(None, 512)]        0           []                               \n","                                                                                                  \n"," Type (InputLayer)              [(None, 512)]        0           []                               \n","                                                                                                  \n"," bert (Bert)                    ((None, 52, 768),    109482241   ['Ids[0][0]',                    \n","                                 (None, 461, 768))                'mask[0][0]',                   \n","                                                                  'Type[0][0]']                   \n","                                                                                                  \n"," att_flow (AttFlow)             ((16, 461, 768),     2304        ['bert[0][1]',                   \n","                                 (16, 461, 768))                  'bert[0][0]']                   \n","                                                                                                  \n"," modelling_input_layer (modelli  (16, 461, 3072)     0           ['bert[0][1]',                   \n"," ng_input_layer)                                                  'att_flow[0][0]',               \n","                                                                  'att_flow[0][1]']               \n","                                                                                                  \n"," modelling_layer (modelling_lay  (16, 461, 200)      2085600     ['modelling_input_layer[0][0]']  \n"," er)                                                                                              \n","                                                                                                  \n"," input_to_start (input_to_start  (16, 461, 3272)     0           ['modelling_input_layer[0][0]',  \n"," )                                                                'modelling_layer[0][0]']        \n","                                                                                                  \n"," input_to_end (input_to_end)    (16, 461, 3272)      181200      ['modelling_input_layer[0][0]',  \n","                                                                  'modelling_layer[0][0]']        \n","                                                                                                  \n"," output_start (output_start)    (16, 461)            3273        ['input_to_start[0][0]']         \n","                                                                                                  \n"," output_end (output_end)        (16, 461)            3273        ['input_to_end[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 111,757,891\n","Trainable params: 2,275,650\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["#defining various parameters of the model and building it\n","\n","#input timesteps\n","# question_timesteps = question_max\n","# context_timesteps = context_max\n","\n","#output dimensions of word and character embedding\n","hidden_size = 100\n","# char_vocab = 1218\n","\n","#character CNN filters and width\n","n_filters = 100\n","filter_width = 3\n","\n","tf.keras.backend.clear_session()\n","# model = bidaf_model(question_timesteps, context_timesteps, hidden_size, char_vocab, n_filters, filter_width)\n","model = bidaf_model(hidden_size,  n_filters, filter_width)\n","\n","print(model.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHfjyz_ZYrZf"},"outputs":[],"source":["#https://medium.com/@aakashgoel12/how-to-add-user-defined-function-get-f1-score-in-keras-metrics-3013f979ce0d\n","def f1_score(y_true, y_pred):    #taken from old keras source code\n","    \n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    \n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    \n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    \n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    \n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    \n","    return f1_val"]},{"cell_type":"markdown","metadata":{"id":"Tf169pp0D_9S"},"source":["## Loading pre trained model from disk for fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25405,"status":"ok","timestamp":1676065244877,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"IhUvxPvdvFKZ","outputId":"6e575700-5680-46b2-df06-3aa2a0b2ea85"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["model = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/modelbase2')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1676065244877,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"zeAMeZogvqAf","outputId":"4ca3e6bf-2273-4d75-ecab-8d27891299e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"bidaf\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Ids (InputLayer)               [(None, 512)]        0           []                               \n","                                                                                                  \n"," mask (InputLayer)              [(None, 512)]        0           []                               \n","                                                                                                  \n"," Type (InputLayer)              [(None, 512)]        0           []                               \n","                                                                                                  \n"," bert (Bert)                    ((None, 52, 768),    109482241   ['Ids[0][0]',                    \n","                                 (None, 461, 768))                'mask[0][0]',                   \n","                                                                  'Type[0][0]']                   \n","                                                                                                  \n"," att_flow (AttFlow)             ((16, 461, 768),     2304        ['bert[0][1]',                   \n","                                 (16, 461, 768))                  'bert[0][0]']                   \n","                                                                                                  \n"," modelling_input_layer (modelli  (16, 461, 3072)     0           ['bert[0][1]',                   \n"," ng_input_layer)                                                  'att_flow[0][0]',               \n","                                                                  'att_flow[0][1]']               \n","                                                                                                  \n"," modelling_layer (modelling_lay  (16, 461, 200)      2085600     ['modelling_input_layer[0][0]']  \n"," er)                                                                                              \n","                                                                                                  \n"," input_to_start (input_to_start  (16, 461, 3272)     0           ['modelling_input_layer[0][0]',  \n"," )                                                                'modelling_layer[0][0]']        \n","                                                                                                  \n"," input_to_end (input_to_end)    (16, 461, 3272)      181200      ['modelling_input_layer[0][0]',  \n","                                                                  'modelling_layer[0][0]']        \n","                                                                                                  \n"," output_start (output_start)    (16, 461)            3273        ['input_to_start[0][0]']         \n","                                                                                                  \n"," output_end (output_end)        (16, 461)            3273        ['input_to_end[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 111,757,891\n","Trainable params: 2,275,650\n","Non-trainable params: 109,482,241\n","__________________________________________________________________________________________________\n","None\n"]}],"source":["print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"-MluMH2ljcYz"},"source":["### Prepare Data Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRtdj4z8jc2I"},"outputs":[],"source":["#defining loss function and optimizer\n","loss_function = tf.keras.losses.CategoricalCrossentropy(reduction='auto')\n","optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nklcTy4vioc6"},"outputs":[],"source":["#batch size\n","BATCH_SIZE=16\n","##number of epochs\n","EPOCHS= 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmCXqUEBYrZx"},"outputs":[],"source":["##creating input dataset(train and test) using tf.data\n","train_inputs = tf.data.Dataset.from_tensor_slices((train_input_id,train_input_mask,train_input_type))\n","test_inputs = tf.data.Dataset.from_tensor_slices((test_input_id,test_input_mask,test_input_type))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l8DdhiFvYrZy"},"outputs":[],"source":["##creating output dataset(train and test) using tf.data\n","train_targets = tf.data.Dataset.from_tensor_slices((y_start_train, y_end_train))\n","test_targets = tf.data.Dataset.from_tensor_slices((y_start_test, y_end_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-RgRwG2YrZ0"},"outputs":[],"source":["#shuffling and split to batches\n","train_dataset = tf.data.Dataset.zip((train_inputs, train_targets)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","test_dataset = tf.data.Dataset.zip((test_inputs, test_targets)).shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"YzL1UgL7jmLc"},"source":["### Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D9Njoz3PYrZ6"},"outputs":[],"source":["#https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn\n","#steps to be performed in each training step\n","@tf.function\n","def train_step(input_vector, output_vector,loss_fn):\n","    with tf.GradientTape() as tape:\n","        #forward propagation\n","        # print(type(input_vector))\n","        # print(type(input_vector[0]))\n","        # print(input_vector[0].shape)\n","        # print(input_vector.shape)\n","        output_predicted = model(inputs=input_vector, training=True)\n","\n","        # print(type(output_predicted))\n","        # print(type(input_vector[0]))\n","        # print(input_vector[0].shape)\n","\n","        #loss\n","        loss_start = loss_function(output_vector[0], output_predicted[0])\n","        loss_end = loss_function(output_vector[1], output_predicted[1])\n","        loss_final = loss_start + loss_end\n","        # print('losses Calculated')\n","    #getting gradients\n","    gradients = tape.gradient(loss_final, model.trainable_variables)\n","    # print('Gradients Calculated')\n","    #applying gradients\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","    # print('Gradients applied')\n","    return loss_start, loss_end, output_predicted, gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Tgpvs5_jc2S"},"outputs":[],"source":["#https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn\n","#steps to be performed in each validation step\n","@tf.function\n","def val_step(input_vector, output_vector, loss_fn):\n","    #getting output of validation data\n","    output_predicted = model(inputs=input_vector, training=False)\n","    #loss calculation\n","    loss_start = loss_function(output_vector[0], output_predicted[0])\n","    loss_end = loss_function(output_vector[1], output_predicted[1])\n","    return loss_start, loss_end, output_predicted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4HATh0Fjc2V"},"outputs":[],"source":["#defining functions to compute the mean loss for each epoch\n","train_start_loss = tf.keras.metrics.Mean(name='train_start_loss')\n","train_end_loss = tf.keras.metrics.Mean(name='train_end_loss')\n","val_start_loss = tf.keras.metrics.Mean(name='val_start_loss')\n","val_end_loss = tf.keras.metrics.Mean(name='val_end_loss')\n","train_start_f1 = tf.keras.metrics.Mean(name=\"train_start_f1\")\n","train_end_f1 = tf.keras.metrics.Mean(name=\"train_end_f1\")\n","val_start_f1 = tf.keras.metrics.Mean(name=\"val_start_f1\")\n","val_end_f1 = tf.keras.metrics.Mean(name=\"val_end_f1\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MDVggVcjc2X"},"outputs":[],"source":["#tensorboard file writers\n","wtrain = tf.summary.create_file_writer(logdir='logdir_train')\n","wval = tf.summary.create_file_writer(logdir='logdir_val')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJN9naSkjc2Y"},"outputs":[],"source":["#to get the iteration number for recording logs\n","iters = math.ceil(64769/BATCH_SIZE) \n","#for model checkpointing\n","best_loss=100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3CsVhntjc2a","outputId":"70465b4d-3249-41d8-88fb-41eac9905786"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["100%|██████████| 150/150 [07:14<00:00,  2.89s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1, Train Start Loss: 0.988486, Start F1 Score: 0.86356, Train End Loss: 1.081917, End F1 Score: 0.84867,\n","    #Val Start Loss: 0.000169, Val Start F1 Score: 1.00000, Val End Loss: 0.000086, Val End F1 Score: 1.00000\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▊         | 13/150 [00:35<06:15,  2.74s/it]"]}],"source":["for epoch in range(EPOCHS):\n","    \n","    #resetting the states of the loss and metrics\n","    train_start_loss.reset_states()\n","    train_end_loss.reset_states()\n","    val_start_loss.reset_states()\n","    val_end_loss.reset_states()\n","    train_start_f1.reset_states()\n","    train_end_f1.reset_states()\n","    val_start_f1.reset_states()\n","    val_end_f1.reset_states()\n","    \n","    ##counter for train loop iteration\n","    counter = 0\n","    \n","    #ietrating over train data batch by batch\n","    for text_seq, label_seq in tqdm(iterable=train_dataset, total=len(list(train_dataset))):\n","        #train step\n","        try:\n","            loss_start_, loss_end_, pred_out, gradients = train_step(text_seq, label_seq, loss_function)\n","        except:\n","            continue\n","        #adding loss to train loss\n","        train_start_loss(loss_start_)\n","        train_end_loss(loss_end_)\n","        #counting the step number\n","        temp_step = epoch*iters+counter\n","        counter = counter + 1\n","        \n","        #calculating f1 for batch\n","        f1_start = f1_score(label_seq[0], pred_out[0])\n","        f1_end = f1_score(label_seq[1], pred_out[1])\n","        train_start_f1(f1_start)\n","        train_end_f1(f1_end)\n","        \n","        # ##tensorboard \n","        # with tf.name_scope('per_step_training'):\n","        #     with wtrain.as_default():\n","        #         tf.summary.scalar(\"start_loss\", loss_start_, step=temp_step)\n","        #         tf.summary.scalar(\"end_loss\", loss_end_, step=temp_step)\n","        #         tf.summary.scalar('f1_start', f1_start, step=temp_step)\n","        #         tf.summary.scalar('f1_end', f1_end, step=temp_step)\n","        # with tf.name_scope(\"per_batch_gradients\"):\n","        #     with wtrain.as_default():\n","        #         for i in range(len(model.trainable_variables)):\n","        #             name_temp = model.trainable_variables[i].name\n","        #             tf.summary.histogram(name_temp, gradients[i], step=temp_step)\n","    \n","    \n","    #validation data\n","    for text_seq_val, label_seq_val in test_dataset:\n","        #getting val output\n","        try:\n","            loss_val_start, loss_val_end, pred_out_val = val_step(text_seq_val, label_seq_val, loss_function)\n","        except:\n","            continue\n","        \n","        val_start_loss(loss_val_start)\n","        val_end_loss(loss_val_end)\n","        \n","        #calculating metric\n","        f1_start_val = f1_score(label_seq_val[0], pred_out_val[0])\n","        f1_end_val = f1_score(label_seq_val[1], pred_out_val[1])\n","        val_start_f1(f1_start_val)\n","        val_end_f1(f1_end_val)\n","    \n","   \n","    #printing\n","    template = '''Epoch {}, Train Start Loss: {:0.6f}, Start F1 Score: {:0.5f}, Train End Loss: {:0.6f}, End F1 Score: {:0.5f},\n","    #Val Start Loss: {:0.6f}, Val Start F1 Score: {:0.5f}, Val End Loss: {:0.6f}, Val End F1 Score: {:0.5f}'''\n","\n","    print(template.format(epoch+1, train_start_loss.result(), train_start_f1.result(), \n","                          train_end_loss.result(), train_end_f1.result(),\n","                          val_start_loss.result(), val_start_f1.result(),\n","                          val_end_loss.result(), val_end_f1.result()))\n","\n","\n","    # if (val_start_loss.result()+val_end_loss.result())<best_loss:\n","    #   model.save(\"drive/My Drive/Colab Notebooks/new_model/model\")\n","    #   best_loss=(val_start_loss.result()+val_end_loss.result())\n","    \n","    # #tensorboard\n","    # with tf.name_scope(\"per_epoch_loss_metric\"):\n","    #     with wtrain.as_default():\n","    #         tf.summary.scalar(\"start_loss\", train_start_loss.result().numpy(), step=epoch)\n","    #         tf.summary.scalar(\"end_loss\", train_end_loss.result().numpy(), step=epoch)\n","    #         tf.summary.scalar('start_f1', train_start_f1.result().numpy(), step=epoch)\n","    #         tf.summary.scalar('end_f1', train_end_f1.result().numpy(), step=epoch)\n","    #     with wval.as_default():\n","    #         tf.summary.scalar(\"start_loss\", val_start_loss.result().numpy(), step=epoch)\n","    #         tf.summary.scalar(\"end_loss\", val_end_loss.result().numpy(), step=epoch)\n","    #         tf.summary.scalar('start_f1', val_start_f1.result().numpy(), step=epoch)\n","    #         tf.summary.scalar('end_f1', val_end_f1.result().numpy(), step=epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dpYi3oCOaBvw","outputId":"70b34202-a09f-492f-e1f2-9d35c4a454da"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:Function `_wrapped_model` contains input name(s) Ids, Type with unsupported characters which will be renamed to ids, type in the SavedModel.\n","WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, dense_layer_call_fn, dense_layer_call_and_return_conditional_losses, dense_1_layer_call_fn while saving (showing 5 of 390). These functions will not be directly callable after loading.\n"]}],"source":["#saving the model to disk\n","model.save(\"/content/gdrive/MyDrive/Colab Notebooks/modelbasemoddata\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLdun-p2yuDd"},"outputs":[],"source":["#saving tensorboard logs\n","%cp logdir_train/*.v2 \"drive/My Drive/Colab Notebooks/logdir/train/\"\n","%cp logdir_val/*.v2 \"drive/My Drive/Colab Notebooks/logdir/val/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":708,"status":"error","timestamp":1675605673934,"user":{"displayName":"Miraj Shah","userId":"06296854211864520356"},"user_tz":-330},"id":"vBaIYfx6gBww","outputId":"5b116035-744d-4ef2-949a-1744a0c26de9"},"outputs":[{"ename":"InvalidArgumentError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-eecc1f58853d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_type\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#   print(\"Question:\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"modelling_input_layer\" (type modelling_input_layer).\n\nGraph execution error:\n\nDetected at node 'concat' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/platform/asyncio.py\", line 149, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 690, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/ioloop.py\", line 743, in _run_callback\n      ret = callback()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 787, in inner\n      self.run()\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 748, in run\n      yielded = self.gen.send(value)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"/usr/local/lib/python3.8/dist-packages/tornado/gen.py\", line 209, in wrapper\n      yielded = next(result)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.8/dist-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2854, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 2881, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3057, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3249, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py\", line 3326, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-7-6fd90eda3f52>\", line 1, in <module>\n      model = tf.keras.models.load_model('/content/gdrive/MyDrive/Colab Notebooks/modelbase')\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/saving/save.py\", line 209, in load_model\n      return saved_model_load.load(filepath_str, compile, options)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/saving/saved_model/load.py\", line 141, in load\n      loaded = tf.__internal__.saved_model.load_partial(\nNode: 'concat'\nConcatOp : Dimension 0 in both shapes must be equal: shape[0] = [1,461,768] vs. shape[1] = [0,461,768]\n\t [[{{node concat}}]] [Op:__inference_restored_function_body_105159]\n\nCall arguments received by layer \"modelling_input_layer\" (type modelling_input_layer):\n  • args=(['tf.Tensor(shape=(1, 461, 768), dtype=float32)', 'tf.Tensor(shape=(0, 461, 768), dtype=float32)', 'tf.Tensor(shape=(0, 461, 768), dtype=float32)'],)\n  • kwargs=<class 'inspect._empty'>"]}],"source":["# def print_predictions(data_point):\n","#   \"\"\"Function that takes record numbers as input and predicts the answer for that record\"\"\"\n","\n","tf.expand_dims(train_input_type[0],axis=0).shape\n","\n","start,end = model([tf.expand_dims(train_input_id[0],axis=0),tf.expand_dims(train_input_mask[0],axis=0),tf.expand_dims(train_input_type[0],axis=0)],training=True)\n","    \n","#   print(\"Question:\")\n","#   for i in test_question_word_padded[data_point]:\n","#     if i==0:\n","#       break\n","#     else:\n","#       print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(i)], end = ' ')\n","#   print(\"\\nContext:\")\n","#   for i in test_context_word_padded[data_point]:\n","#     if i==0:\n","#       break\n","#     else:\n","#       print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(i)], end = ' ')\n","#   print(\"\\nPredicted Answer:\")\n","#   start, end = model.predict([test_question_word_padded[data_point:data_point+1], test_context_word_padded[data_point:data_point+1], test_question_char_padded[15:16], test_context_char_padded[15:16]])\n","#   for i in range(start.argmax(), end.argmax()+1):\n","#     print(list(word_tokenizer.keys())[list(word_tokenizer.values()).index(test_context_word_padded[data_point][i])], end=' ')\n","#   print(\"\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"}},"nbformat":4,"nbformat_minor":0}